To compare the performance of different backends for storing recent observations, a special Python
program has been written.

The storage is a rolling buffer for keeping the latest (typically 24H of) observations for a set of
time series. Currently, three backends have been implemented/compared: two variants based only
on a (Postgres-based) relational database and one variant that keeps the observations in netCDF
files.

An *observation* is assumed to consist of two components:

* a time represented as a *UNIX timestamp* (seconds since 1970-01-01T00:00:00Z)
* a value represented as a *floating point number*

A *time series* is assumed to be identified by the combination of a *station* and a *parameter*
(like air temperature).

==== Storage backends

The following backends are currently implemented:

[cols="1,3"]
|=========================
|*Name*  |*Description*
| TimescaleDBSBE | Keeps all data in a TimescaleDB database extended with PostGIS.
| PostGISSBE | Keeps all data in a Postgres database extended with PostGIS.
| NetCDFSBE_TSMDataInPostGIS | Keeps all data in netCDF files on the local file system, one file per time series. Per time series metadata (i.e. not actual observations) will also be kept in a Postgres database extended with PostGIS to speed up searching for target files to retrieve observations from.
|=========================

*Note:* the program is designed to make it easy to add more backends.

==== Use cases

The following use cases are currently implemented:

[cols="1,3"]
|=========================
|*Name*  |*Description*
|Fill  |Fill storage with observations.
|AddNew |Add new observations to the storage while deleting any old observations and overwriting any existing ones.
|GetAll |Retrieve all observations.
|=========================

*Note:* the program is designed to make it easy to add more use cases.

==== Testing

Testing is done essentially according to this algorithm:

[source]
----
for uc in use cases:
    for sbe in storage backends:
        apply uc to sbe and aggregate stats
----

The synthetic time series generated by the program "sample" observations from an underlying sine wave.

===== Test environment

The tests have been run in the following environment:

* HW: HP ZBook, Intel Core i7-6820HQ CPU @ 2.70GHz Ã— 8, 16GB RAM, 250GB disk
* OS: Ubuntu 18.04 Bionic
* Python version: 3.9

The TimescaleDBSBE backend uses a separate TimescaleDB server running in a local docker container.

The PostGISSBE and NetCDFSBE_TSMDataInPostGIS backends both use a PostGIS server that runs
in a local docker container (they use separate databases, though: `esoh_postgis` for PostGISSBE
and `esoh_netcdf` for NetCDFSBE_TSMDataInPostGIS).

===== Test configuration

The currently relevant configuration settings are these:

[cols="1,3"]
|=========================
|*Name* |*Description*
|`max_age` |Maximum age (in seconds) of an observation in the storage. The effectively limits the capacity of the storage along the time dimension. (**Note**: what is considered the 'current time' is manipulated by the program depending on the particular test!)
|`nstations` |Number of stations to simulate.
|`nparams` |Number of parameters to simulate. (The number of time series will thus be `nstations` * `nparams`)
|`time_res`  |Time series resolution in seconds.
|`extra_secs` |Applicable to the *AddNew* use case, this is the number of seconds worth of new observations to add/merge into the storage.
|=========================

===== Metadata

In addition to the station location, each time series is accompanied with a small amount of dummy
metadata (such as quality of sensor) which currently is not used for anything except taking up space
in the storage. Per observation metadata (such as quality of observation) is currently not used at
all.

===== Test results

*Warning:* The below results were run on 2023-04-26 and based on that version of the test
program. There are two aspects that should be kept in mind when reading the results:

* Only a small number of test runs for each combination were run. The results should thus be considered indicative only.
* Although we have made some efforts to optimize the performance of all backends, we assume there might be further optimization potential for some of them, thus improving the fairness of the comparison.

[#img-storage-test-results]
.Storage test results.
image::poc-experiments/datastore-sqlvsfiles/test_results.png[Storage test results, 1000]

To conclude the testing conducted in this PoC so far, write performance seems to be better when using files directly, while databases seem to outperform direct file access in data query operations. Comparison will give even more interesting results once Elastic is taken into comparison, too. A time search demo is also considered to be added. Geosearches can also be benchmarked, but it's not relevant before Elastic is added into this testbench.
